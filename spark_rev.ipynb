{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## py310\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyPySparkApp2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environemnt check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\")\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python version of Spark Context in the PySpark shell is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\")\n",
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The master of Spark Context in the PySpark shell is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\")\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the current database/catalog using SparkSession\n",
    "tables = spark.catalog.listTables()\n",
    "for table in tables:\n",
    "    print(table.name, table.tableType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with more values and columns\n",
    "data = [\n",
    "    (\"Alice\", 34, \"F\", \"Engineering\", \"London\"),\n",
    "    (\"Bob\", 45, \"M\", \"Marketing\", \"New York\"),\n",
    "    (\"Cathy\", 29, \"F\", \"HR\", \"Sydney\"),\n",
    "    (\"David\", 23, \"M\", \"IT\", \"Tokyo\"),\n",
    "    (\"Ella\", 54, \"F\", \"Finance\", \"Toronto\")\n",
    "]\n",
    "columns = [\"name\", \"age\", \"gender\", \"department\", \"location\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or replace a temporary view\n",
    "df.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop the temporary view named \"people\"\n",
    "# spark.catalog.dropTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees TEMPORARY\n"
     ]
    }
   ],
   "source": [
    "# List all tables in the current database/catalog using SparkSession\n",
    "tables = spark.catalog.listTables()\n",
    "for table in tables:\n",
    "    print(table.name, table.tableType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-----------+--------+\n",
      "| name|age|gender| department|location|\n",
      "+-----+---+------+-----------+--------+\n",
      "|Alice| 34|     F|Engineering|  London|\n",
      "|  Bob| 45|     M|  Marketing|New York|\n",
      "|Cathy| 29|     F|         HR|  Sydney|\n",
      "|David| 23|     M|         IT|   Tokyo|\n",
      "| Ella| 54|     F|    Finance| Toronto|\n",
      "+-----+---+------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the content of the employees table\n",
    "spark.table(\"employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "| Ella| 54|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the temporary view using SQL\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='temp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python list of numbers from 1 to 100 \n",
    "numb = range(1, 100)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Diamond Price Prediction\n",
      "\n",
      "## Overview\n",
      "This project focuses on building and evaluating machine learning models to predict diamond prices based on various features such as carat, cut, color, clarity, and dimensions (x, y, z). The dataset used is cleaned and preprocessed to ensure reliable results, and multiple regression models are implemented and compared to find the best-performing model.\n",
      "\n",
      "---\n",
      "\n",
      "## Features and Dataset\n",
      "The dataset includes the following features:\n",
      "- **Carat**: Weight of the diamond.\n"
     ]
    }
   ],
   "source": [
    "# Show the first few lines of the text file\n",
    "for line in lines.take(10):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|      name|   string|   NULL|\n",
      "|       age|   bigint|   NULL|\n",
      "|    gender|   string|   NULL|\n",
      "|department|   string|   NULL|\n",
      "|  location|   string|   NULL|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE TABLE employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the employees table: ['name', 'age', 'gender', 'department', 'location']\n"
     ]
    }
   ],
   "source": [
    "# Print the column names of the employees table\n",
    "columns = spark.table(\"employees\").columns\n",
    "print(\"Columns in the employees table:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the employees table\n",
    "spark.table(\"employees\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-----------+--------+\n",
      "| name|age|gender| department|location|\n",
      "+-----+---+------+-----------+--------+\n",
      "|Alice| 34|     F|Engineering|  London|\n",
      "|  Bob| 45|     M|  Marketing|New York|\n",
      "|Cathy| 29|     F|         HR|  Sydney|\n",
      "|David| 23|     M|         IT|   Tokyo|\n",
      "| Ella| 54|     F|    Finance| Toronto|\n",
      "+-----+---+------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"Data/diamonds.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|_c0|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1| 0.23|  Ideal|    E|    SI2| 61.5|   55|  326|3.95|3.98|2.43|\n",
      "|  2| 0.21|Premium|    E|    SI1| 59.8|   61|  326|3.89|3.84|2.31|\n",
      "+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='temp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running sums using window function SQL\n",
    "\n",
    "A window function is like an aggregate function, except that it gives an output for every row in the dataset instead of a single row per group.\n",
    "\n",
    "You can do aggregation along with window functions. A running sum using a window function is simpler than what is required using joins. The query duration can also be much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table called schedule, having columns train_id, station, time, and diff_min is provided for you. \n",
    "\n",
    "The diff_min column gives the elapsed time between the current station and the next station on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+\n",
      "|train_id|      station| time|diff_min|\n",
      "+--------+-------------+-----+--------+\n",
      "|     217|       Gilroy|6:06a|     9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|\n",
      "|     217|      Capitol|6:42a|     8.0|\n",
      "|     217|       Tamien|6:50a|     9.0|\n",
      "|     217|     San Jose|6:59a|    NULL|\n",
      "|     324|San Francisco|7:59a|     4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|\n",
      "|     324| Redwood City|8:31a|     6.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|\n",
      "|     324|     San Jose|9:05a|    NULL|\n",
      "+--------+-------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the data\n",
    "data = [\n",
    "    (217, \"Gilroy\", \"6:06a\", 9.0),\n",
    "    (217, \"San Martin\", \"6:15a\", 6.0),\n",
    "    (217, \"Morgan Hill\", \"6:21a\", 15.0),\n",
    "    (217, \"Blossom Hill\", \"6:36a\", 6.0),\n",
    "    (217, \"Capitol\", \"6:42a\", 8.0),\n",
    "    (217, \"Tamien\", \"6:50a\", 9.0),\n",
    "    (217, \"San Jose\", \"6:59a\", None),\n",
    "    (324, \"San Francisco\", \"7:59a\", 4.0),\n",
    "    (324, \"22nd Street\", \"8:03a\", 13.0),\n",
    "    (324, \"Millbrae\", \"8:16a\", 8.0),\n",
    "    (324, \"Hillsdale\", \"8:24a\", 7.0),\n",
    "    (324, \"Redwood City\", \"8:31a\", 6.0),\n",
    "    (324, \"Palo Alto\", \"8:37a\", 28.0),\n",
    "    (324, \"San Jose\", \"9:05a\", None)\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"train_id\", \"station\", \"time\", \"diff_min\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "schedule_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create or replace a temporary view called schedule\n",
    "schedule_df.createOrReplaceTempView(\"schedule\")\n",
    "\n",
    "# Show the table content\n",
    "schedule_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+-------------+\n",
      "|train_id|      station| time|diff_min|running_total|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "|     217|       Gilroy|6:06a|     9.0|          9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|         15.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|         30.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|         36.0|\n",
      "|     217|      Capitol|6:42a|     8.0|         44.0|\n",
      "|     217|       Tamien|6:50a|     9.0|         53.0|\n",
      "|     217|     San Jose|6:59a|    NULL|         53.0|\n",
      "|     324|San Francisco|7:59a|     4.0|          4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|         17.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|         25.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|         32.0|\n",
      "|     324| Redwood City|8:31a|     6.0|         38.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|         66.0|\n",
      "|     324|     San Jose|9:05a|    NULL|         66.0|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time, diff_min,\n",
    "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the row number of the erroneous row as an integer.\n",
    "\n",
    "Provide the clause (as a string) that when added to the OVER clause fixes the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-----+---------+\n",
      "|row|train_id|      station| time|time_next|\n",
      "+---+--------+-------------+-----+---------+\n",
      "|  1|     217|       Gilroy|6:06a|    6:15a|\n",
      "|  2|     217|   San Martin|6:15a|    6:21a|\n",
      "|  3|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|  4|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|  5|     217|      Capitol|6:42a|    6:50a|\n",
      "|  6|     217|       Tamien|6:50a|    6:59a|\n",
      "|  7|     217|     San Jose|6:59a|    7:59a|\n",
      "|  8|     324|San Francisco|7:59a|    8:03a|\n",
      "|  9|     324|  22nd Street|8:03a|    8:16a|\n",
      "| 10|     324|     Millbrae|8:16a|    8:24a|\n",
      "| 11|     324|    Hillsdale|8:24a|    8:31a|\n",
      "| 12|     324| Redwood City|8:31a|    8:37a|\n",
      "| 13|     324|    Palo Alto|8:37a|    9:05a|\n",
      "| 14|     324|     San Jose|9:05a|     NULL|\n",
      "+---+--------+-------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "# Give the number of the bad row as an integer\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause, SQL keywords in upper case\n",
    "clause = 'PARTITION BY train_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
